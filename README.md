# ğŸ“š RAG Document Q&A System

An interactive Streamlit app for uploading documents and asking questions using **Retrieval-Augmented Generation (RAG)**. It processes your documents into chunks, stores them in a vector store, and uses an LLM to answer queries based on the content.

---

## ğŸš€ Features

- ğŸ“¤ Upload multiple document formats: PDF, TXT, CSV, DOCX, DOC
- ğŸ” Automatic document chunking and vector embedding
- ğŸ§  LLM backend options:
  - `local`: Minimal logic-based fallback
  - `ollama`: Local LLMs like Llama2 via [Ollama](https://ollama.ai)
  - `huggingface`: Hosted transformer models via Hugging Face API
- ğŸ’¬ Natural language Q&A from your files
- ğŸ“„ Source documents shown with each answer
- âš™ï¸ Configurable settings for chunking, creativity, and top-k retrieval

---

## ğŸ“‚ Project Structure

```
RAG-QA-App/
â”œâ”€â”€ app.py              # Main Streamlit app
â”œâ”€â”€ rag_pipeline.py     # Core RAG pipeline classes (RAGPipeline, RAGConfig, DocumentProcessor)
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md           # This file
```

---

## ğŸ›  Installation & Setup

### 1. Clone the Repository

```bash
git clone https://github.com/Whateverdoes/RAG.git
cd rag-doc-qa
```

### 2. Create and Activate a Virtual Environment

```bash
python -m venv venv
source venv/bin/activate   # On Windows: venv\Scripts\activate
```

### 3. Install Required Packages

```bash
pip install -r requirements.txt
```

### 4. Run the App

```bash
streamlit run app.py
```

---

## ğŸ§± Tech Stack

- [Streamlit](https://streamlit.io/) â€“ UI framework
- [LangChain](https://www.langchain.com/) â€“ Chunking, embedding, querying
- [Ollama](https://ollama.ai) â€“ Run local LLMs (LLaMA2, Mistral, etc.)
- [Hugging Face Transformers](https://huggingface.co/) â€“ Hosted models
- [FAISS / ChromaDB](https://github.com/facebookresearch/faiss) â€“ Vector DBs

---

## ğŸ”Œ LLM Backend Options

### âœ… Local
Basic logic only; does not use any LLM.

### ğŸ¦™ Ollama (Recommended)

1. Download from: https://ollama.ai
2. Pull a model:
   ```bash
   ollama pull llama2
   ```
3. Select `ollama` from sidebar

### ğŸ¤— Hugging Face

1. Get a token: https://huggingface.co/settings/tokens
2. Enter token in sidebar
3. Choose `huggingface` as backend

---

## âš™ï¸ Config Options

| Setting         | Description                              |
|----------------|------------------------------------------|
| Chunk Size      | Characters per chunk                    |
| Chunk Overlap   | Overlap between chunks                  |
| Temperature     | Response randomness (0.0 = deterministic)|
| Top K           | Number of chunks retrieved              |

---

## ğŸ’¬ Example Questions

- "Summarize the uploaded document."
- "What are the main points covered?"
- "List all mentioned authors."


---

## ğŸ›¡ License

This project is licensed under the MIT License.  
See the [LICENSE](LICENSE) file for more information.

---

## ğŸ¤ Contributing

Pull requests are welcome.  
For suggestions or bug reports, open an issue.

---
